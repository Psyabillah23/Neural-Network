# -*- coding: utf-8 -*-
"""PRAK 1 NN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LdqOCMuU1XuNeEzq3EQh4GBV7RV3Snpf
"""

import numpy as np
import matplotlib.pyplot as plt

# Data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
t = np.array([0, 0, 0, 1])

# Inisialisasi parameter
b = np.float_(0.0)  # bias w0
eta = 0.01
niter = 10
rand = np.random.RandomState()
W = rand.normal(loc=0.0, scale=0.01, size=X.shape[1])
print ("W sebelum training",W)

# Fungsi sederhana untuk menampilkan scatter plot dan decision boundary
def plot_decision_boundary(X, t, W, b):
    plt.scatter(X[:, 0], X[:, 1], c=t, cmap='bwr')
    x_vals = np.linspace(0, 1, 100)
    y_vals = -(W[0] * x_vals + b) / W[1]
    plt.plot(x_vals, y_vals, "--k")
    plt.grid(True)
    plt.show()

# Menampilkan scatter plot dan decision boundary
plot_decision_boundary(X, t, W, b)





import numpy as np
import matplotlib.pyplot as plt

# Data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
t = np.array([0, 0, 0, 1])  # Label target

# Inisialisasi parameter
b = np.float_(0.0)  # Bias
eta = 0.1  # Learning rate
niter = 100  # Jumlah iterasi
W = np.random.normal(loc=0.0, scale=0.01, size=X.shape[1])  # Bobot random
print("W sebelum training:", W)

# Fungsi aktivasi biner untuk model perceptron
def activation(x):
    return np.where(x >= 0.0, 1, 0)

# Fungsi untuk menampilkan scatter plot dan decision boundary
def plot_decision_boundary(X, t, W, b, title):
    plt.scatter(X[:, 0], X[:, 1], c=t, cmap='bwr', edgecolor='k')
    x_vals = np.linspace(-0.2, 1.2, 100)
    y_vals = -(W[0] * x_vals + b) / W[1]  # Menghitung garis keputusan
    plt.plot(x_vals, y_vals, "--k")
    plt.xlim(-0.2, 1.2)
    plt.ylim(-0.2, 1.2)
    plt.title(title)
    plt.grid(True)
    plt.show()

# Menampilkan scatter plot sebelum pelatihan
plot_decision_boundary(X, t, W, b, "Sebelum Training")

# Proses pelatihan menggunakan Perceptron
for _ in range(niter):
    for i in range(X.shape[0]):
        output = activation(np.dot(X[i], W) + b)
        error = t[i] - output
        W += eta * error * X[i]  # Update bobot
        b += eta * error  # Update bias

# Menampilkan W setelah pelatihan
print("W setelah training:", W)

# Menampilkan scatter plot setelah pelatihan
plot_decision_boundary(X, t, W, b, "Setelah Training")

x_vals = np.arange(0,1, 0.2)
print(x_vals)

x_vals_lin = np.linspace(0,1, 100)
print(x_vals_lin)

import numpy as np
import matplotlib.pyplot as plt

X=np.array([[0,0],
            [0,1],
            [1,0],
            [1,1]])

t=np.array([0,0,0,1])

b=np.float_(0.0) #b=w0
eta=0.01
niter=10
rand = np.random.RandomState(1)
W=rand.normal(loc=0.0, scale=0.01, size=X.shape[1])

#training
for _ in range(niter):
    for i in range(X.shape[0]):
     #print(X[i], W)
      a=np.dot(X[i],W)+b
      y=np.where(a>= 0.0, 1, 0)
      delta= eta * (t[i] - y)
      W += delta * X[i]
      b += delta

#testing
for i in range(X.shape[0]):
  a = np.dot(X[i],W)+b
  print(np.where(a >= 0.0, 1, 0))

import numpy as np
import matplotlib.pyplot as plt

# Input data (XOR)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# Target labels
t = np.array([0, 0, 0, 1])

# Bias term initialized
b = np.float_(0.0)  # b = w0

# Learning rate and number of iterations
eta = 0.01
niter = 1000

# Randomly initialized weights with seed for reproducibility
rand = np.random.RandomState(1)
W = rand.normal(loc=0.0, scale=0.01, size=X.shape[1])

# Training the perceptron
for _ in range(niter):
    for i in range(X.shape[0]):
        a = np.dot(X[i], W) + b  # Compute the activation
        y = np.where(a >= 0.0, 1, 0)  # Apply step function
        delta = eta * (t[i] - y)  # Calculate the error
        W += delta * X[i]  # Update the weights
        b += delta  # Update the bias

# Testing the perceptron
for i in range(X.shape[0]):
    a = np.dot(X[i], W) + b
    print(f"Input: {X[i]}, Predicted: {np.where(a >= 0.0, 1, 0)}")

# Function to plot scatter plot with decision boundary
def plot_decision_boundary(X, W, b):
    plt.figure(figsize=(8, 6))

    # Plotting data points
    plt.scatter(X[:, 0], X[:, 1], c=t, cmap='bwr', edgecolor='k', s=100)

    # Defining decision boundary
    x_values = np.linspace(0, 1, 100)
    y_values = -(W[0] * x_values + b) / W[1]  # From w1*x1 + w2*x2 + b = 0

    # Plotting the decision boundary
    plt.plot(x_values, y_values, label="Decision Boundary", color="green")

    plt.xlim(-0.1, 1.1)
    plt.ylim(-0.1, 1.1)
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Perceptron Decision Boundary')
    plt.legend()
    plt.show()

# Call function to plot
plot_decision_boundary(X, W, b)

import numpy as np
import matplotlib.pyplot as plt

# Data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
t = np.array([0, 0, 0, 1])

# Input manual untuk bobot w1 dan w2
w1 = float(input("Masukkan nilai untuk w1 (bobot untuk X1): "))
w2 = float(input("Masukkan nilai untuk w2 (bobot untuk X2): "))

# Inisialisasi bias
b = 0.0  # bias w0
eta = 0.01
niter = 10

# Inisialisasi bobot dari input user
W = np.array([w1, w2])

# Training
for _ in range(niter):
    for i in range(X.shape[0]):
        a = np.dot(X[i], W) + b  # Aktivasi (linier)
        y = np.where(a >= 0.0, 1, 0)  # Fungsi aktivasi (step function)
        delta = eta * (t[i] - y)  # Koreksi error
        W += delta * X[i]  # Update bobot
        b += delta  # Update bias

# Hasil training
print("W:", W)
print("b:", b)

# Fungsi untuk menampilkan scatter plot
def plot_decision_boundary(X, t, W, b):
    plt.scatter(X[:, 0], X[:, 1], c=t, cmap='bwr', marker='o')

    # Plot garis keputusan (decision boundary)
    x_vals = np.array([0, 1])
    y_vals = -(W[0] * x_vals + b) / W[1]
    plt.plot(x_vals, y_vals, "--", color="black")

    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.title('Decision Boundary')
    plt.grid(True)
    plt.show()

# Menampilkan scatter plot dan decision boundary
plot_decision_boundary(X, t, W, b)

import numpy as np
import matplotlib.pyplot as plt

# Input data (XOR)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# Target labels
t = np.array([0, 0, 0, 1])

# Bias term initialized
b = np.float_(0.0)  # b = w0

# Learning rate and number of iterations
eta = 0.01
niter = 100

# Input weights W1 and W2 manually from user
W1 = float(input("Masukkan nilai bobot awal W1: "))
W2 = float(input("Masukkan nilai bobot awal W2: "))
W = np.array([W1, W2])  # Initial weight vector

# Training the perceptron
for _ in range(niter):
    for i in range(X.shape[0]):
        a = np.dot(X[i], W) + b  # Compute the activation
        y = np.where(a >= 0.0, 1, 0)  # Apply step function
        delta = eta * (t[i] - y)  # Calculate the error
        W += delta * X[i]  # Update the weights
        b += delta  # Update the bias

# Testing the perceptron
print("\nHasil prediksi setelah pelatihan:")
for i in range(X.shape[0]):
    a = np.dot(X[i], W) + b
    print(f"Input: {X[i]}, Predicted: {np.where(a >= 0.0, 1, 0)}")

# Function to plot scatter plot with decision boundary
def plot_decision_boundary(X, W, b):
    plt.figure(figsize=(8, 6))

    # Plotting data points
    plt.scatter(X[:, 0], X[:, 1], c=t, cmap='bwr', edgecolor='k', s=100)

    # Defining decision boundary
    x_values = np.linspace(0, 1, 100)
    y_values = -(W[0] * x_values + b) / W[1]  # From w1*x1 + w2*x2 + b = 0

    # Plotting the decision boundary
    plt.plot(x_values, y_values, label="Decision Boundary", color="green")

    plt.xlim(-0.1, 1.1)
    plt.ylim(-0.1, 1.1)
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Perceptron Decision Boundary')
    plt.legend()
    plt.show()

# Call function to plot
plot_decision_boundary(X, W, b)

import numpy as np

# Data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
t = np.array([0, 0, 0, 1])  # AND gate

# Tetapkan nilai awal bobot
w0 = -0.5  # Bias
w1 = 1.0   # Bobot untuk x1
w2 = 1.0   # Bobot untuk x2
eta = 0.01 # Learning rate

# Perceptron Class dengan bobot tetap
class FixedPerceptron:
    def __init__(self, w0, w1, w2):
        self.W = np.array([w1, w2], dtype=float)  # Bobot untuk x1 dan x2
        self.b = float(w0)  # Bias

    def fit(self, X, y, epochs=10, eta=0.01):
        for epoch in range(epochs):
            error_count = 0  # Hitung berapa prediksi yang salah
            for i in range(X.shape[0]):
                z = np.dot(X[i], self.W) + self.b
                yout = np.where(z >= 0.0, 1, 0)
                if yout != y[i]:
                    error_count += 1
                    delta = eta * (y[i] - yout)
                    self.W += delta * X[i]
                    self.b += delta
            print(f"Epoch {epoch + 1}: Errors={error_count}")
            if error_count == 0:  # Jika semua prediksi benar, keluar dari loop
                print(f"Model fit setelah {epoch + 1} epochs")
                break
        else:
            print("Model belum fit, coba tingkatkan jumlah epochs atau ubah learning rate.")
        print(f"Bobot akhir: W1={self.W[0]}, W2={self.W[1]}, Bias={self.b}")

    def predict(self, x):
        z = np.dot(x, self.W) + self.b
        return np.where(z >= 0.0, 1, 0)

# Create perceptron dengan bobot awal yang telah ditetapkan
NN = FixedPerceptron(w0, w1, w2)

# Mainkan epochsnya sampai model fit
NN.fit(X, t, epochs=17)

# Test perceptron dengan data
print("[INFO] testing perceptron...")
for (x, target) in zip(X, t):
    pred = NN.predict(x)
    print(f"data={x}, ground-truth={target}, pred={pred}")